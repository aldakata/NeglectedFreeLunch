{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import create_siamese_dataset\n",
    "from ffcv.writer import DatasetWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToTensor, ToDevice\n",
    "from ffcv.fields.decoders import IntDecoder, NDArrayDecoder, FloatDecoder\n",
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import IntField, FloatField, TorchTensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_records  = np.load('data/mouse_record_interpolated.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr0 = torch.from_numpy(mouse_records).permute(0,2,1)[:1000]\n",
    "mr0 = mr0.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2, 30])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 60])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr0.reshape((len(mr0), 60)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TensorDataset(mr0, torch.ones(len(mr0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8950, 0.7687, 0.6545, 0.6347, 0.6384, 0.6459, 0.6500, 0.6500, 0.6500,\n",
       "          0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500,\n",
       "          0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500,\n",
       "          0.6500, 0.6500, 0.6500],\n",
       "         [0.4933, 0.8799, 0.9792, 0.6603, 0.6312, 0.6135, 0.6133, 0.6133, 0.6133,\n",
       "          0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133,\n",
       "          0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133, 0.6133,\n",
       "          0.6133, 0.6133, 0.6133]], dtype=torch.float64),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/resnet50_losses_0.npy\"\n",
    "tensor_dataset = create_siamese_dataset(path, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 9963.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to data/siamese_dataset.beton\n"
     ]
    }
   ],
   "source": [
    "write_path = \"data/siamese_dataset.beton\"\n",
    "writer = DatasetWriter(write_path, {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    'mr0': TorchTensorField(\n",
    "        dtype=torch.float64,\n",
    "        shape=(30, 2),\n",
    "    ),\n",
    "    # 'mr1': TorchTensorField(\n",
    "    #     dtype=torch.float64,\n",
    "    #     shape=(60, 1),\n",
    "    # ),\n",
    "    'target': IntField(),\n",
    "    # 't0': FloatField(),\n",
    "    # 't1': FloatField(),\n",
    "    # 'w0': FloatField(),\n",
    "    # 'w1': FloatField(),\n",
    "})\n",
    "writer.from_indexed_dataset(td)\n",
    "print(f\"Dataset written to { write_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dataset (`torch.utils.data.Dataset`) of (image, label) pairs\n",
    "my_dataset = np.ones((100, 128))\n",
    "dataset = TensorDataset(torch.from_numpy(my_dataset))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "for i, (b) in enumerate(dataloader):\n",
    "    print(i, b[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 60]) torch.Size([10, 60]) torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import RGBImageField, IntField, FloatField, TorchTensorField\n",
    "\n",
    "# Your dataset (`torch.utils.data.Dataset`) of (image, label) pairs\n",
    "mr0 = np.ones((100, 60))\n",
    "mr1 = np.ones((100, 60))\n",
    "t = np.ones((100, 1), dtype=int)\n",
    "\n",
    "dataset_better = TensorDataset(torch.from_numpy(mr0), torch.from_numpy(mr1), torch.from_numpy(t))\n",
    "dataloader_better = torch.utils.data.DataLoader(dataset_better, batch_size=10)\n",
    "for i, (m0, m1, t_) in enumerate(dataloader_better):\n",
    "    print(i, m0.shape, m1.shape, t_.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " (tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.], dtype=torch.float64),\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.], dtype=torch.float64),\n",
       "  tensor([1])))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_better), dataset_better[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 996.46it/s]\n"
     ]
    }
   ],
   "source": [
    "write_path = \"test.beton\"\n",
    "# Pass a type for each data field\n",
    "writer = DatasetWriter(write_path, {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    'mr0': TorchTensorField(\n",
    "        dtype=torch.float64,\n",
    "        shape=(60, 1),\n",
    "    ),\n",
    "    'mr1': TorchTensorField(\n",
    "        dtype=torch.float64,\n",
    "        shape=(60, 1),\n",
    "    ),\n",
    "    'target': IntField()\n",
    "})\n",
    "\n",
    "# Write dataset\n",
    "writer.from_indexed_dataset(dataset_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToTensor, ToDevice\n",
    "from ffcv.fields.decoders import IntDecoder, NDArrayDecoder\n",
    "\n",
    "# Data decoding and augmentation\n",
    "mr_pipeline = [ NDArrayDecoder(), ToTensor()]\n",
    "target_pipeline = [IntDecoder(), ToTensor()]\n",
    "\n",
    "# Pipeline for each data field\n",
    "pipelines = {\n",
    "    'mr0': mr_pipeline,\n",
    "    'mr1': mr_pipeline,\n",
    "    'target': target_pipeline\n",
    "}\n",
    "\n",
    "# Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\n",
    "loader = Loader(write_path, batch_size=8, num_workers=1,\n",
    "                order=OrderOption.RANDOM, pipelines=pipelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 1]) torch.Size([8, 60, 1]) torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (mr0, mr1, t) in enumerate(loader):\n",
    "    print(mr0.shape, mr1.shape, t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_path = 'test.beton'\n",
    "\n",
    "# Pass a type for each data field\n",
    "writer = DatasetWriter(write_path, {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    'image': RGBImageField(\n",
    "        max_resolution=256\n",
    "    ),\n",
    "    'label': IntField()\n",
    "})\n",
    "\n",
    "# Write dataset\n",
    "writer.from_indexed_dataset(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_path = \"/scratch_local/owl156-462029/NeglectedFreeLunch/data/resnet50_losses_0.npy\",\n",
    "margin=1\n",
    "arr = create_siamese_data(sh_path, margin)\n",
    "np.save(args.destiny_path, arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
