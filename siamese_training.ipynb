{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamese_dataloader import Siamese_dataset_folder, Siamese_dataset, Siamese_dataloader\n",
    "from torch import nn as nn\n",
    "from torch import device, cuda, no_grad, manual_seed, cat\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from numpy import load, logical_not, logical_and, arange\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872249, 218063)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2024\n",
    "\n",
    "root=\"../train_xml\"\n",
    "# Raw data\n",
    "subset = logical_not(load(\"data/missing_file_names_mask.npy\")) # Makes losses adapt to the selected mouse_records\n",
    "losses = load(\"data/resnet50_losses_final_weights.npy\")[subset]\n",
    "mouse_records = load(\"data/mouse_record_interpolated.npy\")\n",
    "\n",
    "# Clean data aka reasonable estimated time\n",
    "estimate_times=load('data/sample_estimate_times.npy')\n",
    "clean_subset = estimate_times[subset] < 3000\n",
    "clean_subset = logical_and(clean_subset, estimate_times[subset] > 0)\n",
    "\n",
    "losses = losses[clean_subset]\n",
    "mouse_records = mouse_records[clean_subset]\n",
    "\n",
    "# Train and test split\n",
    "assert len(losses) == len(mouse_records)\n",
    "indices = list(arange(len(losses)))\n",
    "indices_randomized = np.random.choice(indices, int(len(indices)), replace=False)\n",
    "train_indices = indices_randomized[:int(0.8*len(indices))]\n",
    "test_indices = indices_randomized[int(0.8*len(indices)):]\n",
    "\n",
    "losses_train = losses[train_indices]\n",
    "mouse_records_train = mouse_records[train_indices]\n",
    "dataset_train = Siamese_dataset(losses_train, mouse_records_train, seed=SEED)\n",
    "dataloader_train = Siamese_dataloader(dataset_train, batch_size=4096, num_workers=8, shuffle=True).run()\n",
    "\n",
    "\n",
    "losses_test = losses[test_indices]\n",
    "mouse_records_test = mouse_records[test_indices]\n",
    "dataset_test = Siamese_dataset(losses_test, mouse_records_test, seed=SEED)\n",
    "dataloader_test = Siamese_dataloader(dataset_test, batch_size=4096, num_workers=8, shuffle=True).run()\n",
    "\n",
    "\n",
    "\n",
    "len(dataset_train), len(dataset_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, net, criterion, optimizer, epoch):\n",
    "    running_loss = 0.0\n",
    "    for _, (inputs_0, inputs_1, labels) in enumerate(train_loader, 0):\n",
    "        inputs_0, inputs_1, labels = inputs_0.to(device), inputs_1.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs_0, inputs_1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_network(nn.Module):\n",
    "    # It should predict 0, 1.\n",
    "    # 0 if 1st sample has higher loss\n",
    "    # 1 if 2nd sample has higher loss\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # input is 30x3 dim and output is 1 dim\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(90, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "        )\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.shared_features(x)\n",
    "        y = self.shared_features(y)\n",
    "\n",
    "        return self.classification_head(x * y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "The number of parameters is 23073\n",
      "Training\n",
      "Initial test accuracy is 0.4999289196241453 and loss is 38.65213871002197\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "net = Siamese_network()\n",
    "print(f\"The number of parameters is {sum(p.numel() for p in net.parameters())}\")\n",
    "net.to(device)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.8)\n",
    "\n",
    "print(\"Training\")\n",
    "history = []\n",
    "\n",
    "with no_grad():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "    for _, (inputs_0, inputs_1, labels) in enumerate(dataloader_test, 0):\n",
    "        inputs_0, inputs_1, labels = inputs_0.to(device), inputs_1.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs_0, inputs_1)\n",
    "        predictions.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "    predictions = cat(predictions)\n",
    "    all_labels = cat(all_labels)\n",
    "preds = predictions<0\n",
    "accuracy = (preds == all_labels).sum().item() / len(predictions)\n",
    "print(f\"Initial test accuracy is {accuracy} and loss is {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:20<01:20, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5 | Loss : 152.05445128679276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:35<00:52, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Loss : 151.70633572340012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:52<00:34, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5 | Loss : 151.66187465190887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:12<00:18, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5 | Loss : 151.44574791193008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:31<00:00, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5 | Loss : 151.40130192041397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training, final accuracy is 0.5001903119740625 and loss is 38.43146401643753\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for _, epoch in enumerate(tqdm(range(EPOCHS))):\n",
    "    e_loss = train_one_epoch(dataloader_train, net, criterion, optimizer, epoch)\n",
    "    print(f\"Epoch: {epoch}/{EPOCHS} | Loss : {e_loss}\")\n",
    "    history.append(e_loss)\n",
    "\n",
    "with no_grad():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "    for _, (inputs_0, inputs_1, labels) in enumerate(dataloader_test, 0):\n",
    "        inputs_0, inputs_1, labels = inputs_0.to(device), inputs_1.to(device), labels.to(device)\n",
    "        outputs = net(inputs_0, inputs_1)\n",
    "        predictions.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "    predictions = cat(predictions)\n",
    "    all_labels = cat(all_labels)\n",
    "    preds = predictions<0\n",
    "    accuracy = (preds == all_labels).sum().item() / len(predictions)\n",
    "print(f\"Finished Training, final accuracy is {accuracy} and loss is {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
