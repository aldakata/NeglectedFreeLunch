{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Data loader for LUAB\n",
    "### Convert the dataset into _ffcv_ format .beton\n",
    "Steps to follow:\n",
    "- Load ImageNet-AB\n",
    "- Define fields of the dataset\n",
    "- Write new dataset\n",
    "### Implement _ffcv_ dataloading\n",
    "Steps to follow:\n",
    "- Specify built-in data transformations\n",
    "- Implement custom data transformations\n",
    "- Specify piplines, order, num_workers...\n",
    "- Load the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from timm.data import create_transform\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import wget\n",
    "wget.download(\"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\")\n",
    "\n",
    "wget.download(\"https://huggingface.co/datasets/coallaoh/ImageNet-AB/resolve/main/imagenet_ab_v1_0.tar.gz?download=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    points = []\n",
    "    for obj in root.findall(\"metadata\"):\n",
    "        if obj.find(\"selected\").text == \"True\":\n",
    "            selected_point = obj.find(\"selectedRecord\")\n",
    "            points.append([\n",
    "                float(selected_point.find(\"x\").text),\n",
    "                float(selected_point.find(\"y\").text),\n",
    "            ])\n",
    "\n",
    "    return np.array(points)\n",
    "\n",
    "\n",
    "def get_imagenet_selected_point_info(image_path, xml_root):\n",
    "    \"\"\"\n",
    "    image_path: /img_root/Data/CLS-LOC/train/n15075141/n15075141_9993.JPEG\n",
    "    box_xml: /xml_root/Annotations/CLS-LOC/train/n15075141/n15075141_9993.xml\n",
    "\n",
    "    Returns:\n",
    "      boxes: Numpy array or None\n",
    "      sizes: [width, height, depth] or None\n",
    "      None is returned if no box sup exists.\n",
    "    \"\"\"\n",
    "    fragments = image_path.split(\"/\")\n",
    "    file_name_no_extension = fragments[-1].split('.')[0]\n",
    "    source_xml = os.path.join(xml_root, \"Annotations\", fragments[-4], \"train_enriched\", fragments[-2],\n",
    "                           file_name_no_extension + '.xml')\n",
    "\n",
    "    if not os.path.isfile(source_xml):\n",
    "        return None\n",
    "\n",
    "    return load_points(source_xml)\n",
    "\n",
    "\n",
    "def check_in_point(loc_info, gt_points):\n",
    "    if len(gt_points) != 0:\n",
    "        img_h, img_w = loc_info['img_h'], loc_info['img_w']\n",
    "        is_flip = loc_info['f']\n",
    "        crop_x0 = loc_info['j'] / img_w\n",
    "        crop_y0 = loc_info['i'] / img_h\n",
    "        crop_x1 = crop_x0 + loc_info['w'] / img_w\n",
    "        crop_y1 = crop_y0 + loc_info['h'] / img_h\n",
    "\n",
    "        avg_point = sum(gt_points) / len(gt_points)\n",
    "        if is_flip:\n",
    "            crop_x0, crop_x1 = 1 - crop_x1, 1 - crop_x0\n",
    "            avg_point[0] = 1 - avg_point[0]\n",
    "\n",
    "        return (crop_x0 <= avg_point[0] and crop_x1 >= avg_point[0]) and \\\n",
    "               (crop_y0 <= avg_point[1] and crop_y1 >= avg_point[1]), \\\n",
    "               np.array([(avg_point[0] - crop_x0) * img_w / loc_info['w'], (avg_point[1] - crop_y0) * img_h / loc_info['h']])\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "\n",
    "def compute_cls(original_label, LUAB_points, loc_info, class_gt=False, num_classes=1000,\n",
    "                loss_weight=1):\n",
    "    is_fg = False\n",
    "    weight = np.array(1, dtype=np.float32)\n",
    "    if LUAB_points is not None:\n",
    "        is_fg, fg_point = check_in_point(loc_info=loc_info, gt_points=LUAB_points)\n",
    "                      \n",
    "    if not is_fg or LUAB_points is None:\n",
    "        weight = np.array(0, dtype=np.float32)\n",
    "        fg_point = np.array([-1, -1], dtype=np.float32)\n",
    "\n",
    "    return original_label, weight, fg_point\n",
    "\n",
    "\n",
    "class RRCFlipReturnParams(timm.data.transforms.RandomResizedCropAndInterpolation):\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped and resized.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Randomly cropped and resized image.\n",
    "        \"\"\"\n",
    "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
    "        if isinstance(self.interpolation, (tuple, list)):\n",
    "            interpolation = random.choice(self.interpolation)\n",
    "        else:\n",
    "            interpolation = self.interpolation\n",
    "        processed_img = F.resized_crop(img, i, j, h, w, self.size, interpolation)\n",
    "\n",
    "        f = torch.rand(1) < 0.5\n",
    "        if f:\n",
    "            processed_img = F.hflip(processed_img)\n",
    "        return processed_img, {'i': i, 'j': j, 'h': h, 'w': w, 'f': f, 'img_h': img.size[1], 'img_w': img.size[0]}\n",
    "\n",
    "\n",
    "class ImageNetwithLUAB(torchvision.datasets.folder.ImageFolder):\n",
    "    def __init__(self, root, xml_root, transform=None, pre_transform=None,\n",
    "                 num_classes=1000, loss_weight=1, seed=0):\n",
    "        super(ImageNetwithLUAB, self).__init__(\n",
    "            root,\n",
    "            transform=transform)\n",
    "        self.xml_root = xml_root\n",
    "        self.pre_transform = pre_transform\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "    def get_point_ingredients(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        image_path = self.imgs[index][0].strip()\n",
    "        points = get_imagenet_selected_point_info(image_path, self.xml_root)\n",
    "        sample, loc_info = self.pre_transform(sample)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target, points, loc_info\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, target, points, loc_info = self.get_point_ingredients(index)\n",
    "\n",
    "        target, weight, fg_point = compute_cls(\n",
    "            original_label=target,\n",
    "            LUAB_points=points,\n",
    "            loc_info=loc_info,\n",
    "            num_classes=self.num_classes,\n",
    "            loss_weight=self.loss_weight)\n",
    "\n",
    "        return sample, (target, weight, fg_point, np.array([loc_info['w'], loc_info['h']], dtype=np.float32))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin FFCV\n",
    "### Write Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import RGBImageField, IntField, JSONField\n",
    "\n",
    "BETON_PATH = '/output/path/for/converted/ds.beton' # TODO\n",
    "\n",
    "root_train = '/home/tmp/dataset/ILSVRC2015/' # TODO\n",
    "xml_path = os.path.join(root_train, 'train')\n",
    "\n",
    "input_size = 224\n",
    "batch_size = 8\n",
    "num_workers = 10\n",
    "\n",
    "_, transform_2nd, transform_final = create_transform(\n",
    "        input_size=input_size,\n",
    "        is_training=True,\n",
    "        auto_augment=None,\n",
    "        color_jitter=0,\n",
    "        re_prob=0,  \n",
    "        interpolation='bicubic',\n",
    "        separate=True\n",
    ")\n",
    "\n",
    "# ImageNetwithLUAB inherits from torchvision.datasets.folder.ImageFolder\n",
    "# which is a DataLoader and NOT a torch Dataset.\n",
    "# Will this work? Would it work with a dataset_train.dataset?\n",
    "# https://pytorch.org/vision/main/generated/torchvision.datasets.DatasetFolder.html#torchvision.datasets.DatasetFolder\n",
    "\n",
    "dataset_train = ImageNetwithLUAB(\n",
    "        root=root_train,\n",
    "        xml_root=xml_path,\n",
    "        num_classes=1000,\n",
    "        transform=transforms.Compose([transform_2nd, transform_final]),\n",
    "        pre_transform=RRCFlipReturnParams(\n",
    "            size=input_size,\n",
    "            scale=(0.08, 1),\n",
    "            interpolation='bicubic'),\n",
    "    )\n",
    "write_path = BETON_PATH\n",
    "\n",
    "# Pass a type for each data field\n",
    "writer = DatasetWriter(write_path, {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    'image': RGBImageField(\n",
    "        max_resolution=256\n",
    "    ),\n",
    "    'label': IntField(),\n",
    "    'byproduct_annotations': JSONField(),\n",
    "})\n",
    "\n",
    "# Write dataset\n",
    "writer.from_indexed_dataset(dataset_train)\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import NDArrayDecoder, FloatDecoder, JSON\n",
    "\n",
    "# Deciding ORDERING. \n",
    "# Random is most expensive, but most random. \n",
    "# Quasi_random is a trade off in between.\n",
    "# Sequential is most efficient but not random.\n",
    "\n",
    "from ffcv.loader import OrderOption\n",
    "# Truly random shuffling (shuffle=True in PyTorch)\n",
    "ORDERING = OrderOption.RANDOM\n",
    "# Unshuffled (i.e., served in the order the dataset was written)\n",
    "ORDERING = OrderOption.SEQUENTIAL\n",
    "# Memory-efficient but not truly random loading\n",
    "# Speeds up loading over RANDOM when the whole dataset does not fit in RAM!\n",
    "ORDERING = OrderOption.QUASI_RANDOM\n",
    "\n",
    "# Deciding PIPEPLINE.\n",
    "# A key-value dictionary where the key matches the one used in writing the dataset, \n",
    "# and the value is a sequence of operations to perform on top. JIT-able\n",
    "# transformations will be JITted for fastest experience, and such. Our example could be\n",
    "PIPELINES = {\n",
    "  'covariate': [NDArrayDecoder(), ToTensor(), transforms.Compose([transform_2nd, transform_final])], # How to do the pre_transform random_resize_and_interpolation?\n",
    "  'label': [FloatDecoder(), ToTensor()],\n",
    "  'byprodut_annotations':[] # JSON encoder to do\n",
    "}\n",
    "\n",
    "loader = Loader(BETON_PATH,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                order=ORDERING,\n",
    "                pipelines=PIPELINES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END FFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train = '/home/tmp/dataset/ILSVRC2015/'\n",
    "xml_path = os.path.join(root_train, 'train')\n",
    "\n",
    "input_size = 224\n",
    "batch_size = 8\n",
    "num_workers = 10\n",
    "\n",
    "_, transform_2nd, transform_final = create_transform(\n",
    "        input_size=input_size,\n",
    "        is_training=True,\n",
    "        auto_augment=None,\n",
    "        color_jitter=0,\n",
    "        re_prob=0,  \n",
    "        interpolation='bicubic',\n",
    "        separate=True\n",
    ")\n",
    "# ImageNetwithLUAB inherits from \n",
    "dataset_train = ImageNetwithLUAB(\n",
    "        root=root_train,\n",
    "        xml_root=xml_path,\n",
    "        num_classes=1000,\n",
    "        transform=transforms.Compose([transform_2nd, transform_final]),\n",
    "        pre_transform=RRCFlipReturnParams(\n",
    "            size=input_size,\n",
    "            scale=(0.08, 1),\n",
    "            interpolation='bicubic'),\n",
    "    )\n",
    "# write_path = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tmp/dataset/ILSVRC2015/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      9\u001b[0m _, transform_2nd, transform_final \u001b[38;5;241m=\u001b[39m create_transform(\n\u001b[1;32m     10\u001b[0m         input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[1;32m     11\u001b[0m         is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         separate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mImageNetwithLUAB\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxml_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransform_2nd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_final\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRRCFlipReturnParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbicubic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     31\u001b[0m         dataset_train,\n\u001b[1;32m     32\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m, in \u001b[0;36mImageNetwithLUAB.__init__\u001b[0;34m(self, root, xml_root, transform, pre_transform, num_classes, loss_weight, seed)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, xml_root, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m              num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, loss_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImageNetwithLUAB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxml_root \u001b[38;5;241m=\u001b[39m xml_root\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform \u001b[38;5;241m=\u001b[39m pre_transform\n",
      "File \u001b[0;32m~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=300'>301</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=301'>302</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=302'>303</a>\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=306'>307</a>\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=307'>308</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=308'>309</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=309'>310</a>\u001b[0m         root,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=310'>311</a>\u001b[0m         loader,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=311'>312</a>\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=312'>313</a>\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=313'>314</a>\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=314'>315</a>\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=315'>316</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=316'>317</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=133'>134</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=134'>135</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=135'>136</a>\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=140'>141</a>\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=141'>142</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=142'>143</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=143'>144</a>\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=144'>145</a>\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=190'>191</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=191'>192</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=192'>193</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=193'>194</a>\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=215'>216</a>\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=216'>217</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=217'>218</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=35'>36</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=36'>37</a>\u001b[0m \n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=37'>38</a>\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=38'>39</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=39'>40</a>\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=40'>41</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/datasets/folder.py?line=41'>42</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tmp/dataset/ILSVRC2015/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_train = '/home/tmp/dataset/ILSVRC2015/'\n",
    "xml_path = os.path.join(root_train, 'train')\n",
    "\n",
    "input_size = 224\n",
    "batch_size = 8\n",
    "num_workers = 10\n",
    "\n",
    "_, transform_2nd, transform_final = create_transform(\n",
    "        input_size=input_size,\n",
    "        is_training=True,\n",
    "        auto_augment=None,\n",
    "        color_jitter=0,\n",
    "        re_prob=0,  \n",
    "        interpolation='bicubic',\n",
    "        separate=True\n",
    ")\n",
    "\n",
    "dataset_train = ImageNetwithLUAB(\n",
    "        root=root_train,\n",
    "        xml_root=xml_path,\n",
    "        num_classes=1000,\n",
    "        transform=transforms.Compose([transform_2nd, transform_final]),\n",
    "        pre_transform=RRCFlipReturnParams(\n",
    "            size=input_size,\n",
    "            scale=(0.08, 1),\n",
    "            interpolation='bicubic'),\n",
    "    )\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the image and annotated point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(img, text, height, width,\n",
    "          font=cv2.FONT_HERSHEY_PLAIN,\n",
    "          pos=(0, 0),              \n",
    "          font_scale=2,\n",
    "          font_thickness=2,\n",
    "          text_color=(255, 255, 255),\n",
    "          text_color_bg=(0, 0, 0),\n",
    "          ):\n",
    "\n",
    "    x, y = pos\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    if x + text_w > width:\n",
    "        x = x - text_w//2\n",
    "    if y + text_h > height:\n",
    "        y = y - text_h//2\n",
    "    cv2.rectangle(img, (x,y), (x + text_w, y + text_h), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (x, y + text_h + font_scale - 1), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    return text_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './metadata/imagenet1000_clsidx_to_labels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(value[i:i\u001b[38;5;241m+\u001b[39mlv\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m16\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, lv, lv\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rgb[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./metadata/imagenet1000_clsidx_to_labels.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     13\u001b[0m class_idx_to_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;28mstr\u001b[39m(data_list))\n",
      "File \u001b[0;32m~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=300'>301</a>\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=301'>302</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=302'>303</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=303'>304</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=304'>305</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=305'>306</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/ffcv/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=307'>308</a>\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './metadata/imagenet1000_clsidx_to_labels.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./metadata/imagenet1000_clsidx_to_labels.txt', 'r') as f:\n",
    "    data_list = f.read()\n",
    "\n",
    "class_idx_to_label = eval(str(data_list))\n",
    "it = iter(train_loader)\n",
    "img, target = next(it)\n",
    "img, target = next(it)\n",
    "\n",
    "img_idx = 0\n",
    "points = target[2][img_idx] * np.array([224, 224])\n",
    "img = ((t2n(img[img_idx]).copy() * timm.data.constants.IMAGENET_DEFAULT_STD + timm.data.constants.IMAGENET_DEFAULT_MEAN))*255\n",
    "if target[1][0] > 0:\n",
    "    blend = cv2.circle(img, (int(points[0]), int(points[1])), thickness=-1, lineType=8, radius=5, color=hex_to_rgb(\"526dfe\"))\n",
    "    blend = cv2.circle(blend, (int(points[0]), int(points[1])), thickness=2, lineType=8, radius=5, color=(255,255,255))\n",
    "    draw_text(img=img, text=class_idx_to_label[target[0][img_idx].item()], width=224, height=224, pos=(int(points[0]), int(points[1])+10),\n",
    "                     text_color=(255,255,255),\n",
    "                     text_color_bg=(0,0,0),\n",
    "                     font_scale=1,\n",
    "                     font_thickness=1)    \n",
    "plt.imshow(img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
