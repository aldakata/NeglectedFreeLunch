{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from siamese_dataloader import Siamese_dataset_folder, Siamese_dataloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = \"/scratch_local/owl156-429256/train_xml/\"\n",
    "dataset = Siamese_dataset_folder(\n",
    "    root=xml_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workers: 2\tBatch size: 2\tTime per it: 1.2263450310565531, Total time: 217.13852478155962min\n",
      "Workers: 2\tBatch size: 4\tTime per it: 0.24271376803517342, Total time: 21.487635290504485min\n",
      "Workers: 2\tBatch size: 8\tTime per it: 0.25634929502848536, Total time: 11.347399455624977min\n",
      "Workers: 2\tBatch size: 16\tTime per it: 0.2555995499715209, Total time: 5.657105852158743min\n",
      "Workers: 2\tBatch size: 32\tTime per it: 0.26383750699460506, Total time: 2.9197170045965564min\n",
      "Workers: 2\tBatch size: 64\tTime per it: 0.28400668501853943, Total time: 1.571458048390147min\n",
      "Workers: 2\tBatch size: 128\tTime per it: 0.2812245920067653, Total time: 0.7780321235843766min\n",
      "Workers: 2\tBatch size: 256\tTime per it: 0.4260090780444443, Total time: 0.5892954547324366min\n",
      "Workers: 2\tBatch size: 512\tTime per it: 0.5907715029316023, Total time: 0.40860509717438837min\n",
      "Workers: 2\tBatch size: 1024\tTime per it: 0.9024367340607569, Total time: 0.3120836462023159min\n",
      "Workers: 2\tBatch size: 2048\tTime per it: 1.6027761730365455, Total time: 0.2771386698082721min\n",
      "Workers: 2\tBatch size: 4096\tTime per it: 2.980770457070321, Total time: 0.2577049663494805min\n",
      "Workers: 4\tBatch size: 2\tTime per it: 0.4846068050246686, Total time: 85.80522126917549min\n",
      "Workers: 4\tBatch size: 4\tTime per it: 0.4482098419684917, Total time: 39.68035969198874min\n",
      "Workers: 4\tBatch size: 8\tTime per it: 0.44913209904916584, Total time: 19.88100390792138min\n",
      "Workers: 4\tBatch size: 16\tTime per it: 0.45916138403117657, Total time: 10.162477019139882min\n",
      "Workers: 4\tBatch size: 32\tTime per it: 0.4885425509419292, Total time: 5.406380653389426min\n",
      "Workers: 4\tBatch size: 64\tTime per it: 0.489651019917801, Total time: 2.7093236770185296min\n",
      "Workers: 4\tBatch size: 128\tTime per it: 0.5247435270575806, Total time: 1.4517482905049204min\n",
      "Workers: 4\tBatch size: 256\tTime per it: 0.6125493899453431, Total time: 0.8473353970552203min\n",
      "Workers: 4\tBatch size: 512\tTime per it: 0.7154387399787083, Total time: 0.494830766922025min\n",
      "Workers: 4\tBatch size: 1024\tTime per it: 1.1145385470008478, Total time: 0.3854333943343646min\n",
      "Workers: 4\tBatch size: 2048\tTime per it: 1.784369447035715, Total time: 0.3085382621212228min\n",
      "Workers: 4\tBatch size: 4096\tTime per it: 3.0929965240648016, Total time: 0.26740756345813965min\n",
      "Workers: 8\tBatch size: 2\tTime per it: 0.9026075779693201, Total time: 159.8170767390475min\n",
      "Workers: 8\tBatch size: 4\tTime per it: 0.8895437329774722, Total time: 78.75198619306941min\n",
      "Workers: 8\tBatch size: 8\tTime per it: 0.8289875740883872, Total time: 36.69545159422089min\n",
      "Workers: 8\tBatch size: 16\tTime per it: 0.8616819930030033, Total time: 19.07134126746402min\n",
      "Workers: 8\tBatch size: 32\tTime per it: 0.8484109590062872, Total time: 9.388808786566425min\n",
      "Workers: 8\tBatch size: 64\tTime per it: 0.8392166439443827, Total time: 4.643530659791617min\n",
      "Workers: 8\tBatch size: 128\tTime per it: 0.8741233120672405, Total time: 2.4183376421999503min\n",
      "Workers: 8\tBatch size: 256\tTime per it: 0.9080070429481566, Total time: 1.256039955135804min\n",
      "Workers: 8\tBatch size: 512\tTime per it: 1.0900539220310748, Total time: 0.7539320812303936min\n",
      "Workers: 8\tBatch size: 1024\tTime per it: 1.660668186028488, Total time: 0.5742977463870215min\n",
      "Workers: 8\tBatch size: 2048\tTime per it: 2.55284429108724, Total time: 0.4414165140221531min\n",
      "Workers: 8\tBatch size: 4096\tTime per it: 4.441616851952858, Total time: 0.3840036453175278min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/owl156-429256/scratch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workers: 16\tBatch size: 2\tTime per it: 1.6453055019956082, Total time: 291.320305844526min\n",
      "Workers: 16\tBatch size: 4\tTime per it: 1.6578850969672203, Total time: 146.77383407451265min\n",
      "Workers: 16\tBatch size: 8\tTime per it: 1.6503362740622833, Total time: 73.05276550813832min\n",
      "Workers: 16\tBatch size: 16\tTime per it: 1.756293787038885, Total time: 38.871507644965504min\n",
      "Workers: 16\tBatch size: 32\tTime per it: 1.6926147510530427, Total time: 18.731059610040923min\n",
      "Workers: 16\tBatch size: 64\tTime per it: 1.7437670859508216, Total time: 9.6485645102205min\n",
      "Workers: 16\tBatch size: 128\tTime per it: 1.8865948671009392, Total time: 5.219427647915717min\n",
      "Workers: 16\tBatch size: 256\tTime per it: 1.9842641419963911, Total time: 2.744819066379238min\n",
      "Workers: 16\tBatch size: 512\tTime per it: 2.4097724029561505, Total time: 1.6667108721255575min\n",
      "Workers: 16\tBatch size: 1024\tTime per it: 2.997761241043918, Total time: 1.0366956743207876min\n",
      "Workers: 16\tBatch size: 2048\tTime per it: 4.4010122790932655, Total time: 0.7609862870166145min\n",
      "Workers: 16\tBatch size: 4096\tTime per it: 6.963190598064102, Total time: 0.6020083815923282min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/owl156-429256/scratch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workers: 32\tBatch size: 2\tTime per it: 3.564886234002188, Total time: 631.204202946396min\n",
      "Workers: 32\tBatch size: 4\tTime per it: 3.6485971660586074, Total time: 323.0130942340037min\n",
      "Workers: 32\tBatch size: 8\tTime per it: 3.48701428307686, Total time: 154.35401908613034min\n",
      "Workers: 32\tBatch size: 16\tTime per it: 3.641845021978952, Total time: 80.60383044018599min\n",
      "Workers: 32\tBatch size: 32\tTime per it: 3.5055375481024384, Total time: 38.79348962183643min\n",
      "Workers: 32\tBatch size: 64\tTime per it: 3.4834261080250144, Total time: 19.274398393372106min\n",
      "Workers: 32\tBatch size: 128\tTime per it: 3.6956750369863585, Total time: 10.224404190921875min\n",
      "Workers: 32\tBatch size: 256\tTime per it: 3.8089561379747465, Total time: 5.268903070534005min\n",
      "Workers: 32\tBatch size: 512\tTime per it: 4.330565263982862, Total time: 2.995220710086645min\n",
      "Workers: 32\tBatch size: 1024\tTime per it: 5.294208419974893, Total time: 1.8308606078412686min\n",
      "Workers: 32\tBatch size: 2048\tTime per it: 8.257175656035542, Total time: 1.4277618523311792min\n",
      "Workers: 32\tBatch size: 4096\tTime per it: 12.938497662893496, Total time: 1.118608478193911min\n"
     ]
    }
   ],
   "source": [
    "num_workers = [2,4,8,16,32]\n",
    "batch_sizes = [2,4,8,16,32,64,128,256,512,1024, 2048, 4096]\n",
    "for w in num_workers:\n",
    "    for b in batch_sizes:\n",
    "        loader = Siamese_dataloader(\n",
    "            dataset=dataset,\n",
    "            batch_size=b,\n",
    "            num_workers=w,\n",
    "        ).run()\n",
    "        s = timeit.default_timer()\n",
    "        _ = next(iter(loader))    \n",
    "        f = timeit.default_timer()\n",
    "        print(f\"Workers: {w}\\tBatch size: {b}\\tTime per it: {f-s}, Total time: {((f-s)*len(dataset)/b)/3600}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19920 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = Siamese_dataloader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    ").run()\n",
    "mouse_record_history = []\n",
    "loader = tqdm(loader)\n",
    "\n",
    "for i, (_, _, mr, _) in enumerate(loader):\n",
    "    mouse_record_history.append(mr)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
